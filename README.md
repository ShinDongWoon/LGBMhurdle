# LGBM Hurdle Model for Time Series Forecasting

[![Build Status](https://img.shields.io/badge/build-passing-brightgreen)](https://github.com/shindongwoon/lgbmhurdle)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python Version](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![wSMAPE Score](https://img.shields.io/badge/wSMAPE-0.555008-orange)](.)


## ğŸ“œ Project Overview

This project is engineered to tackle the complex challenge of **forecasting time series data characterized by intermittent demand**. Intermittent demand, where sales data is dominated by frequent zero values, poses a significant hurdle for traditional forecasting models. Standard algorithms often struggle to accurately predict sporadic, non-zero events.

To address this, we employ a sophisticated **Hurdle Model** architecture. This approach strategically decomposes the forecasting problem into two distinct, more manageable sub-problems:
1.  A **Binary Classification** task to predict *whether* a sale will occur (i.e., hurdle the zero-demand barrier).
2.  A **Regression** task to predict *how much* will be sold, conditioned on a sale actually happening.

Both models are implemented using **LightGBM**, a high-performance gradient boosting framework renowned for its speed and accuracy, ensuring a robust and efficient solution.

---

## ğŸš€ Key Features

-   **Hurdle Model Architecture**: By decoupling the prediction of sale occurrence from sale quantity, the model architecture is specifically tailored to handle zero-inflated datasets, maximizing predictive accuracy for intermittent demand patterns.
-   **High-Performance LightGBM Engine**: Utilizes LightGBM as the backbone for both classification and regression tasks, leveraging its exceptional speed and state-of-the-art performance in gradient boosting.
-   **Advanced Feature Engineering**: Implements a comprehensive suite of engineered features to capture complex temporal dynamics:
    -   **Calendar-based Features**: Extracts signals from timestamps (e.g., day of week, week of year, month, quarter).
    -   **Lag and Rolling Window Statistics**: Incorporates historical trends and patterns using lagged values and rolling statistical aggregations (mean, std, min, max).
    -   **Fourier Transform Features**: Precisely models complex seasonality (e.g., weekly, yearly cycles) by transforming temporal features into the frequency domain.
-   **Recursive Forecasting Pipeline**: Employs a recursive, multi-step forecasting strategy. For long prediction horizons, the model uses its own previous predictions as inputs to generate features for subsequent steps, ensuring stable and accurate long-term forecasts.
-   **Efficient Caching System**: A built-in caching mechanism serializes the results of computationally expensive feature engineering steps. This dramatically reduces runtime during iterative experiments and model tuning, accelerating the development cycle.
-   **Configuration-Driven Workflow**: The entire pipeline is managed via `YAML` configuration files. This approach decouples logic from parameters, ensuring experimental **reproducibility** and simplifying hyperparameter tuning and feature selection.

---

## ğŸ“Š Data Schema Definition

For the model to operate correctly, the input and output data must adhere to the following specifications.

### 1. Input Data Schema

All source data for training (`train.csv`) and inference (`TEST_*.csv`) must be structured in a **long format**. Each row should represent a single observation for a specific item on a specific date.

**Required Features:**

| Column Name       | Data Type           | Description                                                                                             |
| :---------------- | :------------------ | :------------------------------------------------------------------------------------------------------ |
| `Date`            | `Date` or `String`  | The date of the sales record (e.g., `YYYY-MM-DD`).                                                      |
| `Series_Identifier` | `String`            | A unique identifier for each distinct time series (e.g., a combination of store and item ID).         |
| `Sales_Quantity`  | `Numeric` (Integer) | The actual quantity sold on that date. This serves as the **target variable** for the model to predict. |

### 2. Output Data Schema

The final prediction output (`sample_submission.csv`) must be pivoted into a **wide-format** structure. In this format, each time series (`Series_Identifier`) becomes a separate column.

**Output Table Structure:**

-   **Index**: The rows are indexed by `Date`.
-   **Column Headers**: The first column is `Date`, followed by columns for every unique `Series_Identifier`.
-   **Values**: Each cell contains the **predicted `Sales_Quantity`** for the corresponding `Date` (row) and `Series_Identifier` (column).

**Example Structure:**

| Date       | StoreA_ItemX | StoreA_ItemY | StoreB_ItemZ | ... |
| :--------- | :----------- | :----------- | :----------- | :-- |
| 2025-09-16 | 15           | 0            | 21           | ... |
| 2025-09-17 | 12           | 3            | 18           | ... |
| ...        | ...          | ...          | ...          | ... |

This standard format facilitates easy analysis and submission of time series forecasting results.

---

## âš™ï¸ Code Pipeline Analysis

The codebase is modularly structured into three core stages: **Data Preprocessing & Feature Engineering**, **Model Training**, and **Recursive Inference**.

### 1. End-to-End Pipeline Flow

1.  **Configuration Loading (`configs/`)**: The pipeline initializes by loading hyperparameters, file paths, and feature definitions from `YAML` configuration files (e.g., `base.yaml`, `korean.yaml`).
2.  **Data Ingestion & Preprocessing (`fe/preprocess.py`)**: Raw data is loaded, and initial cleaning is performed, including data type casting and handling of missing values.
3.  **Feature Engineering (`fe/`)**: A rich set of features is constructed from the preprocessed data.
    -   `calendar.py`: Generates calendar-based features.
    -   `lags_rolling.py`: Creates lag and rolling window statistical features.
    -   `fourier.py`: Produces Fourier term features for seasonality modeling.
4.  **Model Training (`pipeline/train.py`)**:
    -   **Data Splitting**: Utilizes Time Series Cross-Validation (`cv/tscv.py`) to create robust training and validation splits that respect the temporal order of the data.
    -   **Classifier Training (`model/classifier.py`)**: A LightGBM classifier is trained on a binary target (1 if sales > 0, else 0).
    -   **Regressor Training (`model/regressor.py`)**: A LightGBM regressor is trained exclusively on data where sales > 0 to predict the actual sales quantity.
5.  **Model Serialization**: The trained classifier and regressor model artifacts are saved to disk.
6.  **Inference (`pipeline/predict.py` & `recursion.py`)**:
    -   The pipeline iterates one step at a time over the prediction horizon.
    -   **Classification Prediction**: The classifier predicts the probability of a sale occurring on the next day.
    -   **Hurdle Application**: If the predicted probability exceeds a predefined threshold, the regressor predicts the sales quantity. Otherwise, the prediction is set to 0.
    -   **Recursive Update**: The new prediction is used to update the lag and rolling features for the subsequent day's forecast. This loop continues until the entire prediction horizon is covered.
7.  **Submission Generation**: The final predictions are formatted to match the required `sample_submission.csv` schema.

### 2. Module Contributions to Performance

| Module Path                       | Core Function                   | Contribution to Performance                                                                                                                                                             |
| :-------------------------------- | :------------------------------ | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **`g2_hurdle/fe/`** | **Feature Engineering** | Fundamentally improves model accuracy by translating complex temporal patterns (trends, seasonality, autocorrelation) into a format the model can learn. Lag and rolling features are critical. |
| **`g2_hurdle/utils/cache.py`** | **Feature Caching** | Drastically reduces experiment turnaround time by caching the results of expensive feature computations. This accelerates development and hyperparameter tuning by avoiding redundant work.     |
| **`g2_hurdle/model/`** | **Hurdle Model Implementation** | Directly addresses the intermittent demand problem by splitting it into classification and regression. This prevents the regression model from being biased by zero-inflation, significantly improving the final wSMAPE score. |
| **`g2_hurdle/cv/tscv.py`** | **Time Series Cross-Validation**| Ensures a robust and reliable evaluation of the model's generalization performance by preventing data leakage (i.e., training on future data).                                          |
| **`g2_hurdle/pipeline/recursion.py`** | **Recursive Forecasting Logic** | Enables accurate multi-step forecasting by dynamically updating features at each step with the latest available information (i.e., previous predictions). This yields more stable and precise long-range predictions. |
| **`g2_hurdle/configs/`** | **Configuration Management** | Guarantees experimental reproducibility and flexibility by externalizing all hyperparameters and settings. This allows for rapid, code-free iteration on different model configurations.  |

---

## ğŸ“Š Results

Rigorous evaluation of this codebase demonstrated a highly competitive performance, achieving a **wSMAPE score of 0.5550080421**. This result validates the effectiveness of the Hurdle Model architecture combined with sophisticated feature engineering for solving intermittent demand forecasting problems.

-   **wSMAPE (Weighted Symmetric Mean Absolute Percentage Error)**: An industry-standard metric that weights errors based on the actual demand volume, preventing high-percentage errors on low-volume items from disproportionately penalizing the model's score.

---

## ğŸš€ Getting Started on Colab

### 1. Prerequisites

-   Python 3.8+
-   Nvidia GPU (Recommended for training)

### 2. Quickstart (Colab)

```bash
# 1. Clone the repository
!git clone [https://github.com/shindongwoon/lgbmhurdle.git](https://github.com/shindongwoon/lgbmhurdle.git)
%cd lgbmhurdle

# 2. Install dependencies
!python dependency.py

# 3. Run model training
!python train.py

# 4. Run prediction
!python predict.py
## ğŸ“œ í”„ë¡œì íŠ¸ ê°œìš” (Project Overview)

ë³¸ í”„ë¡œì íŠ¸ëŠ” **ê°„í—ì  ìˆ˜ìš”(Intermittent Demand)** íŠ¹ì„±ì„ ê°€ì§„ ì‹œê³„ì—´ ë°ì´í„°ì˜ ë¯¸ë˜ íŒë§¤ëŸ‰ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ê°„í—ì  ìˆ˜ìš”ë€ '0' ê°’ì´ ë¹ˆë²ˆí•˜ê²Œ ë‚˜íƒ€ë‚˜ëŠ” ë°ì´í„°ë¥¼ ì˜ë¯¸í•˜ë©°, ì¼ë°˜ì ì¸ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ë¡œëŠ” ì •í™•í•œ ì˜ˆì¸¡ì´ ì–´ë µìŠµë‹ˆë‹¤.

ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë³¸ í”„ë¡œì íŠ¸ì—ì„œëŠ” **í—ˆë“¤ ëª¨ë¸(Hurdle Model)** ì ‘ê·¼ë²•ì„ ì±„íƒí–ˆìŠµë‹ˆë‹¤. í—ˆë“¤ ëª¨ë¸ì€ 'íŒë§¤ê°€ ë°œìƒí• ì§€ ì—¬ë¶€'ë¥¼ ì˜ˆì¸¡í•˜ëŠ” **ì´ì§„ ë¶„ë¥˜(Binary Classification)** ë¬¸ì œì™€ 'íŒë§¤ê°€ ë°œìƒí–ˆì„ ë•Œ ì–¼ë§ˆë‚˜ íŒ”ë¦´ì§€'ë¥¼ ì˜ˆì¸¡í•˜ëŠ” **íšŒê·€(Regression)** ë¬¸ì œë¡œ ë‚˜ëˆ„ì–´ ì ‘ê·¼í•©ë‹ˆë‹¤. ë‘ ëª¨ë¸ ëª¨ë‘ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ìë‘í•˜ëŠ” **LightGBM**ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.

---

## ğŸš€ ì£¼ìš” íŠ¹ì§• (Key Features)

- **í—ˆë“¤ ëª¨ë¸ ì•„í‚¤í…ì²˜ (Hurdle Model Architecture)**: íŒë§¤ ë°œìƒ ì—¬ë¶€ì™€ íŒë§¤ëŸ‰ì„ ë¶„ë¦¬í•˜ì—¬ ì˜ˆì¸¡í•¨ìœ¼ë¡œì¨ '0'ì´ ë§ì€ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
- **ê³ ì„±ëŠ¥ LightGBM í™œìš© (High-Performance LightGBM)**: ë¹ ë¥¸ í•™ìŠµ ì†ë„ì™€ ë†’ì€ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ìë‘í•˜ëŠ” LightGBMì„ ë¶„ë¥˜ ë° íšŒê·€ ëª¨ë¸ì˜ ë°±ë³¸ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
- **ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (Advanced Feature Engineering)**:
    - **ì‹œê°„ ê¸°ë°˜ í”¼ì²˜**: ë‚ ì§œ, ìš”ì¼, ì›”, ì£¼ì°¨ ë“± ì‹œê°„ ì •ë³´ë¥¼ í™œìš©í•œ í”¼ì²˜
    - **ì§€ì—° ë° ë¡¤ë§ í†µê³„ í”¼ì²˜**: ê³¼ê±° ë°ì´í„°ì˜ ì¶”ì„¸ì™€ íŒ¨í„´ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ Lag ë° Rolling window í”¼ì²˜
    - **í‘¸ë¦¬ì— ë³€í™˜ í”¼ì²˜**: ê³„ì ˆì„±(Seasonality)ì„ ì •êµí•˜ê²Œ ëª¨ë¸ë§í•˜ê¸° ìœ„í•œ Fourier-transform í”¼ì²˜
- **ì¬ê·€ì  ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ (Recursive Prediction Pipeline)**: ì˜ˆì¸¡ ëŒ€ìƒ ê¸°ê°„ì´ ê¸¸ì–´ì§ˆ ë•Œ, ì´ì „ ì˜ˆì¸¡ê°’ì„ ë‹¤ì‹œ í”¼ì²˜ë¡œ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ì‹œì ì˜ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ì¬ê·€ì  êµ¬ì¡°ë¥¼ ì±„íƒí•˜ì—¬ ì•ˆì •ì ì¸ ì¥ê¸° ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- **íš¨ìœ¨ì ì¸ ìºì‹± ì‹œìŠ¤í…œ (Efficient Caching System)**: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ê³¼ì •ì—ì„œ ìƒì„±ëœ ë°ì´í„°ë¥¼ ìºì‹±í•˜ì—¬ ë°˜ë³µì ì¸ í•™ìŠµ ë° ì‹¤í—˜ ì‹œì˜ ì—°ì‚° ë¹„ìš©ì„ íšê¸°ì ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.
- **ì„¤ì • íŒŒì¼ ê¸°ë°˜ ê´€ë¦¬ (Configuration File Management)**: `YAML` í˜•ì‹ì˜ ì„¤ì • íŒŒì¼ì„ í†µí•´ ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°, í”¼ì²˜ ëª©ë¡ ë“±ì„ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•˜ì—¬ ì‹¤í—˜ì˜ ì¬í˜„ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.

---
<img width="512" height="512" alt="Gemini_Generated_Image_2m0zym2m0zym2m0z" src="https://github.com/user-attachments/assets/8bec8402-4d52-429c-873b-f8426e368281" />


<img width="512" height="512" alt="Gemini_Generated_Image_2m0zyn2m0zyn2m0z" src="https://github.com/user-attachments/assets/680ad867-937b-4b2f-9418-93eca84c9a7b" />

<img width="512" height="512" alt="Gemini_Generated_Image_2m0zyo2m0zyo2m0z" src="https://github.com/user-attachments/assets/d05af1d9-7bcb-455d-8465-ef54b263a38d" />

---
## ğŸ“Š ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì •ì˜ (Data Schema Definition)

ë³¸ ì˜ˆì¸¡ ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ê¸° ìœ„í•´ì„œëŠ” ì…ë ¥ ë°ì´í„°ì™€ ì¶œë ¥ ë°ì´í„°ê°€ ë‹¤ìŒ ëª…ì„¸ì— ë”°ë¼ ì •í™•í•˜ê²Œ êµ¬ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

### 1. ì…ë ¥ ë°ì´í„° ìŠ¤í‚¤ë§ˆ (Input Data Schema)

í•™ìŠµ(`train.csv`) ë° í‰ê°€(`TEST_*.csv`)ì— ì‚¬ìš©ë˜ëŠ” ëª¨ë“  ì›ë³¸ ë°ì´í„°ëŠ” **Long-Format** ë°ì´í„° êµ¬ì¡°ë¥¼ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤. ê° í–‰ì€ íŠ¹ì • ì˜ì—…ì¼ì˜ íŠ¹ì • ìƒí’ˆì— ëŒ€í•œ íŒë§¤ ê¸°ë¡ì„ ë‚˜íƒ€ë‚´ëŠ” ê°œë³„ ê´€ì¸¡ì¹˜(Observation)ì…ë‹ˆë‹¤.

**í•„ìˆ˜ í”¼ì²˜(Features):**

| ì»¬ëŸ¼ëª… (Column Name) | ë°ì´í„° íƒ€ì… (Data Type) | ì„¤ëª… (Description)                                                               |
| :------------------- | :---------------------- | :------------------------------------------------------------------------------- |
| `ì˜ì—…ì¼ì`           | `Date` or `String`      | ë§¤ì¶œì´ ë°œìƒí•œ ë‚ ì§œì…ë‹ˆë‹¤. (ì˜ˆ: `YYYY-MM-DD`)                                       |
| `ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…`      | `String`                | ê° ìƒí’ˆ(Item)ì„ ê³ ìœ í•˜ê²Œ ì‹ë³„í•˜ëŠ” **ì‹ë³„ì(Identifier)**ì…ë‹ˆë‹¤.                        |
| `ë§¤ì¶œìˆ˜ëŸ‰`           | `Numeric` (Integer)     | í•´ë‹¹ ì˜ì—…ì¼ì— ë°œìƒí•œ ìƒí’ˆì˜ ì‹¤ì œ íŒë§¤ëŸ‰ìœ¼ë¡œ, ëª¨ë¸ì´ ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” **íƒ€ê²Ÿ ë³€ìˆ˜(Target Variable)**ì…ë‹ˆë‹¤. |

### 2. ì¶œë ¥ ë°ì´í„° ìŠ¤í‚¤ë§ˆ (Output Data Schema)

ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ë¬¼ì¸ `sample_submission.csv` íŒŒì¼ì€ ì…ë ¥ ë°ì´í„°ì™€ ë‹¬ë¦¬ **Wide-Format** ë°ì´í„° êµ¬ì¡°ë¡œ í”¼ë´‡(Pivot)ëœ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤. ì´ëŠ” ê° ìƒí’ˆ(`ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…`)ì´ ê°œë³„ì ì¸ ì»¬ëŸ¼ì´ ë˜ëŠ” ë§¤íŠ¸ë¦­ìŠ¤(Matrix) êµ¬ì¡°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

**ì¶œë ¥ í…Œì´ë¸” êµ¬ì¡°:**

-   **ì¸ë±ìŠ¤ (Index):** í…Œì´ë¸”ì˜ í–‰(Row)ì€ `ì˜ì—…ì¼ì`ê°€ ê¸°ì¤€ì´ ë©ë‹ˆë‹¤.
-   **ì»¬ëŸ¼ í—¤ë” (Column Headers):** í…Œì´ë¸”ì˜ ì²« ë²ˆì§¸ ì—´ì€ `ì˜ì—…ì¼ì`ì´ë©°, ë‘ ë²ˆì§¸ ì—´ë¶€í„°ëŠ” `ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…`ì˜ ëª¨ë“  ê³ ìœ ê°’(Unique Values)ì´ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ìœ„ì¹˜í•©ë‹ˆë‹¤.
-   **ê°’ (Values):** í…Œì´ë¸”ì˜ ê° ì…€(Cell)ì—ëŠ” í•´ë‹¹ `ì˜ì—…ì¼ì`(í–‰)ì™€ `ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…`(ì—´)ì— í•´ë‹¹í•˜ëŠ” **ì˜ˆì¸¡ëœ `ë§¤ì¶œìˆ˜ëŸ‰`**ì´ ê¸°ì…ë©ë‹ˆë‹¤.

**êµ¬ì¡° ì˜ˆì‹œ:**

| ì˜ì—…ì¼ì   | StoreA_ItemX | StoreA_ItemY | StoreB_ItemZ | ... |
| :--------- | :----------- | :----------- | :----------- | :-- |
| 2025-09-16 | 15           | 0            | 21           | ... |
| 2025-09-17 | 12           | 3            | 18           | ... |
| ...        | ...          | ...          | ...          | ... |

ì´ëŸ¬í•œ êµ¬ì¡°ëŠ” ê° ìƒí’ˆì˜ ì¼ë³„ ì˜ˆì¸¡ íŒë§¤ëŸ‰ì„ í•œëˆˆì— íŒŒì•…í•˜ê¸° ìš©ì´í•œ í‘œì¤€ì ì¸ ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼ ì œì¶œ í˜•ì‹ì…ë‹ˆë‹¤.

---

## âš™ï¸ ì½”ë“œ íŒŒì´í”„ë¼ì¸ ë¶„ì„ (Code Pipeline Analysis)

ë³¸ ì½”ë“œë² ì´ìŠ¤ëŠ” í¬ê²Œ **ë°ì´í„° ì „ì²˜ë¦¬ ë° í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§**, **ëª¨ë¸ í•™ìŠµ**, **ì¬ê·€ì  ì˜ˆì¸¡**ì˜ ì„¸ ë‹¨ê³„ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

### 1. ì „ì²´ ì½”ë“œ íŒŒì´í”„ë¼ì¸ íë¦„

1.  **ì´ˆê¸° ì„¤ì • (`configs/`)**: `base.yaml`, `korean.yaml` ë“±ì˜ ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ í”„ë¡œì íŠ¸ ì „ë°˜ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°, ë°ì´í„° ê²½ë¡œ, ì‚¬ìš©í•  í”¼ì²˜ ëª©ë¡ ë“±ì„ ì •ì˜í•©ë‹ˆë‹¤.
2.  **ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ (`fe/preprocess.py`)**: ì›ë³¸ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³ , ê¸°ë³¸ì ì¸ ë°ì´í„° íƒ€ì… ë³€í™˜ ë° ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë“±ì˜ ì „ì²˜ë¦¬ ê³¼ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
3.  **í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (`fe/`)**: ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ë  ë‹¤ì–‘í•œ í”¼ì²˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    - `calendar.py`: ì‹œê°„ ê´€ë ¨ í”¼ì²˜ ìƒì„±
    - `lags_rolling.py`: Lag ë° Rolling í†µê³„ í”¼ì²˜ ìƒì„±
    - `fourier.py`: ê³„ì ˆì„± í”¼ì²˜ ìƒì„±
4.  **ëª¨ë¸ í•™ìŠµ (`pipeline/train.py`)**:
    - **ë°ì´í„° ë¶„í• **: ì‹œê³„ì—´ ë°ì´í„°ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ Time Series Cross-Validation ë°©ì‹ìœ¼ë¡œ í•™ìŠµ/ê²€ì¦ ë°ì´í„°ë¥¼ ë¶„í• í•©ë‹ˆë‹¤ (`cv/tscv.py`).
    - **ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ (`model/classifier.py`)**: íƒ€ê²Ÿ ê°’ì´ 0ì¸ì§€ ì•„ë‹Œì§€ë¥¼ ë ˆì´ë¸”ë¡œ í•˜ì—¬ LightGBM ë¶„ë¥˜ê¸°ë¥¼ í•™ìŠµì‹œí‚µë‹ˆë‹¤.
    - **íšŒê·€ ëª¨ë¸ í•™ìŠµ (`model/regressor.py`)**: íƒ€ê²Ÿ ê°’ì´ 0ì´ ì•„ë‹Œ ë°ì´í„°ë§Œì„ ëŒ€ìƒìœ¼ë¡œ, ì‹¤ì œ íŒë§¤ëŸ‰ì„ ì˜ˆì¸¡í•˜ëŠ” LightGBM íšŒê·€ ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.
5.  **ëª¨ë¸ ì €ì¥**: í•™ìŠµëœ ë¶„ë¥˜ê¸°ì™€ íšŒê·€ê¸° ëª¨ë¸ ê°ì²´ë¥¼ ì§€ì •ëœ ê²½ë¡œì— ì €ì¥í•©ë‹ˆë‹¤.
6.  **ì˜ˆì¸¡ (`pipeline/predict.py` & `recursion.py`)**:
    - í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ í•˜ë£¨(one-step)ì”© ì˜ˆì¸¡ì„ ì§„í–‰í•©ë‹ˆë‹¤.
    - **ë¶„ë¥˜ê¸° ì˜ˆì¸¡**: ë‚´ì¼ íŒë§¤ê°€ ë°œìƒí•  í™•ë¥ ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    - **í—ˆë“¤ ì ìš©**: ì˜ˆì¸¡ëœ í™•ë¥ ì´ ì‚¬ì „ì— ì •ì˜ëœ ì„ê³„ê°’(Threshold)ì„ ë„˜ìœ¼ë©´ íšŒê·€ ëª¨ë¸ì„ í†µí•´ íŒë§¤ëŸ‰ì„ ì˜ˆì¸¡í•˜ê³ , ë„˜ì§€ ì•Šìœ¼ë©´ 0ìœ¼ë¡œ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    - **ì¬ê·€ ì—…ë°ì´íŠ¸**: ì˜ˆì¸¡ëœ ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ë‚  ì˜ˆì¸¡ì— í•„ìš”í•œ Lag, Rolling í”¼ì²˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  ì´ ê³¼ì •ì„ ì˜ˆì¸¡ ê¸°ê°„ì´ ëë‚  ë•Œê¹Œì§€ ë°˜ë³µí•©ë‹ˆë‹¤.
7.  **ê²°ê³¼ ì œì¶œ**: ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ë¥¼ `sample_submission.csv` í˜•ì‹ì— ë§ì¶”ì–´ ìƒì„±í•©ë‹ˆë‹¤.

### 2. ê° ë¶€ë¶„ì˜ ê¸°ëŠ¥ ë° ì„±ëŠ¥ í–¥ìƒ ê¸°ì—¬

| ëª¨ë“ˆ ê²½ë¡œ (Module Path) | í•µì‹¬ ê¸°ëŠ¥ (Core Function) | ì„±ëŠ¥ í–¥ìƒ ê¸°ì—¬ ë°©ì‹ (Contribution to Performance) |
| :--- | :--- | :--- |
| **`g2_hurdle/fe/`** | **í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§** | ì‹œê³„ì—´ ë°ì´í„°ì˜ ë³µì¡í•œ íŒ¨í„´(ì¶”ì„¸, ê³„ì ˆì„±, ìê¸°ìƒê´€ì„±)ì„ ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ **ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ê·¼ë³¸ì ìœ¼ë¡œ í–¥ìƒ**ì‹œí‚µë‹ˆë‹¤. íŠ¹íˆ Lag, Rolling í”¼ì²˜ëŠ” ì‹œê³„ì—´ ì˜ˆì¸¡ì˜ í•µì‹¬ì…ë‹ˆë‹¤. |
| **`g2_hurdle/utils/cache.py`** | **í”¼ì²˜ ìºì‹±** | ëŒ€ìš©ëŸ‰ ë°ì´í„°ì— ëŒ€í•œ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì€ ë§ì€ ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤. ìƒì„±ëœ í”¼ì²˜ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ì¬ì‚¬ìš©í•¨ìœ¼ë¡œì¨, ë°˜ë³µ ì‹¤í—˜ ì‹œ **ì „ì²´ ì‹¤í–‰ ì‹œê°„ì„ ê·¹ì ìœ¼ë¡œ ë‹¨ì¶•**ì‹œì¼œ ê°œë°œ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤. |
| **`g2_hurdle/model/`** | **í—ˆë“¤ ëª¨ë¸ êµ¬í˜„** | **Classifier**ì™€ **Regressor**ë¡œ ì—­í• ì„ ë¶„ë‹´í•˜ì—¬ ê°„í—ì  ìˆ˜ìš” ë¬¸ì œì— íŠ¹í™”ëœ ì ‘ê·¼ì„ í•©ë‹ˆë‹¤. ì´ëŠ” ë‹¨ì¼ íšŒê·€ ëª¨ë¸ì´ '0' ê°’ì— ì˜í•´ í•™ìŠµì´ ì™œê³¡ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê³ , ë‘ ê°€ì§€ ë¬¸ì œë¥¼ ê°ê° ìµœì í™”í•˜ì—¬ **wSMAPE ì ìˆ˜ë¥¼ í¬ê²Œ ê°œì„ **í•©ë‹ˆë‹¤. |
| **`g2_hurdle/cv/tscv.py`** | **ì‹œê³„ì—´ êµì°¨ ê²€ì¦** | ë¯¸ë˜ì˜ ë°ì´í„°ê°€ ê³¼ê±° ë°ì´í„°ì˜ í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” ê²ƒì„ ë°©ì§€(Data Leakage ë°©ì§€)í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë³´ë‹¤ **ì •í™•í•˜ê³  ì‹ ë¢°ì„± ìˆê²Œ í‰ê°€**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. |
| **`g2_hurdle/pipeline/recursion.py`** | **ì¬ê·€ì  ì˜ˆì¸¡ ë¡œì§** | ë‹¤ì¤‘ ì‹œì  ì˜ˆì¸¡(Multi-step Forecasting) ì‹œ, ë§¤ ì‹œì ë§ˆë‹¤ ìµœì‹  ì •ë³´ë¥¼ ë°˜ì˜í•œ í”¼ì²˜ë¥¼ ìƒì„±í•˜ì—¬ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ëŠ” ë‹¨ìˆœíˆ ëª¨ë¸ í•˜ë‚˜ë¡œ ì „ì²´ ê¸°ê°„ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ **ì •êµí•˜ê³  ì•ˆì •ì ì¸ ì¥ê¸° ì˜ˆì¸¡**ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. |
| **`g2_hurdle/configs/`** | **ì„¤ì • ê´€ë¦¬** | ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ì„¤ì •ì„ ì½”ë“œê°€ ì•„ë‹Œ ì™¸ë¶€ íŒŒì¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ê´€ë¦¬í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ **ì‹¤í—˜ì˜ ì¬í˜„ì„±ì„ í™•ë³´**í•˜ê³ , ì½”ë“œ ìˆ˜ì • ì—†ì´ ë‹¤ì–‘í•œ ì¡°ê±´ìœ¼ë¡œ ì†ì‰½ê²Œ ì‹¤í—˜ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. |

---

## ğŸ“Š ì‹¤í—˜ ê²°ê³¼ (Results)

ë³¸ ì½”ë“œë² ì´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ê´€ì ìœ¼ë¡œ ì„±ëŠ¥ì„ ê²€ì¦í•œ ê²°ê³¼, **wSMAPE ì ìˆ˜ 0.5550080421**ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” í—ˆë“¤ ëª¨ë¸ê³¼ ì •êµí•œ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì´ ê°„í—ì  ìˆ˜ìš” ì˜ˆì¸¡ ë¬¸ì œì— ë§¤ìš° íš¨ê³¼ì ì„ì„ ì…ì¦í•˜ëŠ” ê²°ê³¼ì…ë‹ˆë‹¤.

- **wSMAPE (Weighted Symmetric Mean Absolute Percentage Error)**: ìˆ˜ìš”ëŸ‰ì˜ í¬ê¸°ì— ë”°ë¼ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” í‰ê°€ì§€í‘œë¡œ, ìˆ˜ìš”ê°€ ì ì€ í•­ëª©ì˜ ì˜¤ì°¨ì— ê³¼ë„í•œ í˜ë„í‹°ë¥¼ ì£¼ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤.

---

## ğŸš€ Colab ì—ì„œ ì‹œì‘í•˜ê¸° (Getting Started)

### 1. ìš”êµ¬ì‚¬í•­ (Prerequisites)

- Python 3.8+
- Nvidia GPU

### 2. Quickstart (Colab)

```bash
# 1. ì €ì¥ì†Œ í´ë¡ 
!git clone [https://github.com/shindongwoon/lgbmhurdle.git](https://github.com/shindongwoon/lgbmhurdle.git)
cd lgbmhurdle

# 2. ì˜ì¡´ì„± ì„¤ì¹˜
!python dependency.py

# 3. ëª¨ë¸ í›ˆë ¨ ì§„í–‰
!python train.py

# 4. ëª¨ë¸ ì˜ˆì¸¡ ì§„í–‰
!python predict.py
```
All imports are relative; drop this folder as project root and run the commands.

By default, both scripts load the configuration from `g2_hurdle/configs/korean.yaml`.
`train.py` reads `data/train.csv` and stores model artifacts in `./artifacts`.
`predict.py` consumes the artifacts, expects test files in `data/test` with a
`data/sample_submission.csv`, and writes predictions to `outputs/submission.csv`.

## GPU configuration

To enable GPU acceleration, set `runtime.use_gpu` to `true` in the YAML
configuration. The pipeline will automatically set `device_type: gpu` for the
LightGBM models. The older `device` parameter is deprecated and should not be
used.

## Data configuration

Columns used by the toolkit can be provided directly in the `data` section of the
YAML config:

```yaml
data:
  date_col: ds
  target_col: y
  id_cols: [series_id]
```

If these keys are omitted, `resolve_schema` falls back to the corresponding
`*_col_candidates` lists to infer column names.

To clip negative values before feature engineering, list the columns under
`non_negative_cols`:

```yaml
data:
  non_negative_cols: [sales]
```

Any negative values in these columns will be replaced with zero during both
training and prediction.
